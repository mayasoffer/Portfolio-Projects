{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Movies Data Scraper\n",
    "\n",
    " This notebook scrapes data about movies from the site https://www.the-numbers.com/ using Python 3.\n",
    " \n",
    " Also, it uploads it into Data Frame using only relevant data.\n",
    " \n",
    " Lastly, the Data Frame is downloaded as Pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First scraping The data from every movie page (more then 16k pages) and saving each page as a local HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_movies_by_year(year, DIR):\n",
    "    url = f\"https://www.the-numbers.com/market/{year}/top-grossing-movies\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser', from_encoding='utf-8')\n",
    "    soup = soup.find_all('tr')\n",
    "    soup.pop(0)\n",
    "    soup.pop(-1)\n",
    "    soup.pop(-1)\n",
    "    \n",
    "    year_dir = f\"{DIR}\\\\{year}\"\n",
    "    os.makedirs(year_dir)\n",
    "    \n",
    "    for item in soup:\n",
    "        scrape_movie_info(item, year_dir)\n",
    "        \n",
    "        \n",
    "def scrape_movie_info(item, year_dir):\n",
    "    movie_url = f\"https://www.the-numbers.com{item.a['href']}\"\n",
    "    movie_r = requests.get(movie_url)\n",
    "    movie_soup = BeautifulSoup(movie_r.content, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "    with open(f'{year_dir}\\\\{item.td.text}.txt','w',  encoding=\"utf-8\") as f:\n",
    "        f.write(str(movie_soup))\n",
    "        \n",
    "        \n",
    "\n",
    "DIR = f\"{os.path.abspath(os.path.curdir)}\\\\movies_data\" \n",
    "\n",
    "years = range(1995, 2022)    \n",
    "for year in years:\n",
    "    scrape_all_movies_by_year(year, DIR)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the downloaded HTML files and inserting the results into a Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = f\"{os.path.abspath(os.path.curdir)}\\\\movies_data\"\n",
    "\n",
    "def get_soup(year, file):\n",
    "    with open(f\"{DIR}\\\\{year}\\\\{file}\", 'r' , encoding='utf-8') as g:\n",
    "        file_txt = g.read()\n",
    "        file_txt = file_txt.replace(\"\\xa0\", \" \")\n",
    "        return BeautifulSoup(file_txt, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "def get_title(soup):\n",
    "    return soup.div.h1.text.split(r' (')[0]\n",
    "\n",
    "def table_to_dict(table):\n",
    "    row_list = table.find_all('tr')\n",
    "    cols_by_row = [row.find_all('td') for row in row_list]\n",
    "\n",
    "    result = {}\n",
    "    for row in cols_by_row:\n",
    "        if len(row) >= 2:\n",
    "            key = row[0].text\n",
    "            value = row[1].text\n",
    "            #print(key,value)\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "def get_table_by_title(soup, title):\n",
    "    title_element = soup.find_all(\"h2\", string=title)[0]\n",
    "\n",
    "    for sibling in title_element.next_siblings:\n",
    "        # Some of the siblings are not a Tag, but a NavigableString. Filter them out.\n",
    "        if isinstance(sibling, Tag):\n",
    "            # Sometimes the next table is the direct next sibling of the title element,\n",
    "            # and sometimes, it's nested in one of its next siblings.\n",
    "            candidate_tables = [sibling] if sibling.name == \"table\" else []\n",
    "            candidate_tables += sibling.find_all(\"table\")\n",
    "            for candidate_table in candidate_tables:\n",
    "                # filter out weird empty tables that have no content\n",
    "                # (e.g after \"Metrics\" there is a weird table \"movie_ratings\")\n",
    "                if candidate_table.find(\"td\"):\n",
    "                    return candidate_table\n",
    "    raise Exception(f\"Could not find {title} table\")\n",
    "        \n",
    "\n",
    "def get_financial_details_table(soup):\n",
    "    return soup.find_all('table', id=\"movie_finances\")[0]\n",
    "\n",
    "def get_metrics_table(soup):\n",
    "    return get_table_by_title(soup, \"Metrics\")\n",
    "\n",
    "def get_more_details_table(soup):\n",
    "    return get_table_by_title(soup, \"Movie Details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.DataFrame()\n",
    "\n",
    "for year in range(1995, 2022):\n",
    "    print(f\"Year: {year}\")\n",
    "    for file in os.listdir(f\"{DIR}\\\\{year}\\\\\"):\n",
    "        soup = get_soup(year, file)\n",
    "            \n",
    "        #we will create dictionary for every movie that only has title in the start:\n",
    "        movie_dict={ 'title': get_title(soup)}\n",
    "\n",
    "        #uploading table with financial data:\n",
    "        financial_data_dict = table_to_dict(get_financial_details_table(soup))\n",
    "        movie_dict.update(financial_data_dict)\n",
    "\n",
    "        #uploading table with metrics:\n",
    "        metrics_dict = table_to_dict(get_metrics_table(soup))\n",
    "        movie_dict.update(metrics_dict)\n",
    "\n",
    "        #uploading table with more info:\n",
    "        more_details_dict = table_to_dict(get_more_details_table(soup))\n",
    "        movie_dict.update(more_details_dict)\n",
    "        \n",
    "        df_movies = df_movies.append(movie_dict, ignore_index=True)\n",
    "        \n",
    "    \n",
    "\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, performing initial cleaning of the data and saving the Movies Data Frame locally as a Pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning column names\n",
    "df_movies.columns = df_movies.columns.str.strip(r':')\n",
    "#saving to pickle\n",
    "df_movies.to_pickle(f'{os.path.abspath(os.path.curdir)}\\movies.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
